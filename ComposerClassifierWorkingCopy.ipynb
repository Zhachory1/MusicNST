{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ComposerClassifierWorkingCopy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhachory1/MusicNST/blob/master/ComposerClassifierWorkingCopy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAXl3DTkakp7",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "676aa424-6c24-4338-b726-907e03985ffe"
      },
      "source": [
        "#@title Install Dependencies\n",
        "!pip install py-midi pretty_midi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: py-midi in /usr/local/lib/python3.6/dist-packages (2.0.1)\n",
            "Requirement already satisfied: pretty_midi in /usr/local/lib/python3.6/dist-packages (0.2.8)\n",
            "Requirement already satisfied: pyserial in /usr/local/lib/python3.6/dist-packages (from py-midi) (3.4)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.16.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My35Y_cqby4G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1274d891-34c7-4629-9f2a-4946fc035f6f"
      },
      "source": [
        "#@Title Import Data from GitHub \n",
        "!cd MusicNST/ && git pull\n",
        "!git clone https://github.com/Zhachory1/MusicNST.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n",
            "fatal: destination path 'MusicNST' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlHsR9q_cxoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b481860e-76c6-46de-ccd1-9df15f5b4cf0"
      },
      "source": [
        "#@Title Imports\n",
        "import os\n",
        "import fnmatch\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import warnings\n",
        "import midi\n",
        "import pretty_midi\n",
        "import pandas as pd\n",
        "import collections\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "# Note sequence to piano roll\n",
        "import magenta.music.pianoroll_encoder_decoder as pianoroll_ed\n",
        "import magenta.music.sequences_lib as seq_lib\n",
        "import magenta.music as mm\n",
        "import magenta.models.music_vae.data as data\n",
        "\n",
        "from keras import optimizers\n",
        "from keras import backend, losses\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense, Lambda, BatchNormalization, Reshape, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, AveragePooling2D, MaxPooling2D, Convolution1D\n",
        "from keras import backend as K\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils.layer_utils import convert_all_kernels_in_model  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVl6pywidnUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@Title Load Data \n",
        "\n",
        "################################################################################\n",
        "#                                 Constants \n",
        "################################################################################\n",
        "BATCH_SIZE = 20\n",
        "NUM_ITER = 25\n",
        "DF_FNAME = 'midi_filename'\n",
        "DF_FEATURES = 'midi_features'\n",
        "DF_COMPOSER = 'canonical_composer'\n",
        "DF_SPLIT = \"split\"\n",
        "DF_COMPOSER_INDEX = 'composer_idx'\n",
        "PATH_PREFIX = './MusicNST/midi_files/maestro-v1.0.0/'\n",
        "MAX_DIM = 500\n",
        "COUNT_CUTTING = 0\n",
        "################################################################################\n",
        "#                    Helper Functions to manipulate input data. \n",
        "################################################################################\n",
        "\n",
        "def midi_to_pianoroll(midi_filename):\n",
        "  \"\"\"Converts Midi Files into np.arrays and concats them.\n",
        "  \n",
        "  The arrays have the following types:\n",
        "  active, weights, onsets, onset_velocities, active_velocities, offsets, \n",
        "  control_changes\n",
        "  \n",
        "  Args:\n",
        "    midi_filename: string, filename\n",
        "  Returns: \n",
        "    np.array - train tensor.  \n",
        "  \"\"\"\n",
        "  midi_file = pretty_midi.PrettyMIDI(midi_filename)\n",
        "  note_seq = mm.midi_to_sequence_proto(midi_file)\n",
        "  pnt = seq_lib.sequence_to_pianoroll(\n",
        "      note_seq, 10, data.MIN_MIDI_PITCH, data.MAX_MIDI_PITCH)\n",
        "  t_list = [pnt.active, pnt.weights, pnt.onsets, \n",
        "                                pnt.onset_velocities, pnt.active_velocities, \n",
        "                                pnt.offsets, pnt.control_changes]\n",
        "  fin_list = [np.expand_dims(a, axis=-1) for a in t_list]\n",
        "  final_array = np.concatenate(t_list, axis=-1)\n",
        "  return final_array\n",
        "\n",
        "def pianoroll_to_notes(pianoroll, opt_midi_file_name=\"\"):\n",
        "  \"\"\"Helper to obtain note_seq.\"\"\"\n",
        "  note_seq = seq_lib.pianoroll_to_note_sequence(piano_roll, 10, 0)\n",
        "  if opt_midi_file_name != \"\":\n",
        "    download(note_seq, opt_midi_file_name)\n",
        "  return note_seq\n",
        "\n",
        "def load_midi_data_from_midi_files(filenames):\n",
        "  \"\"\"Loads files.\n",
        "  Args:\n",
        "    filenames: list of strings, midi filenames\n",
        "  Returns: \n",
        "    pandas dataframe.\"\"\"\n",
        "  tensor_dict = collections.OrderedDict()\n",
        "  index_array = []\n",
        "  i = 0\n",
        "  for mn in filenames:\n",
        "    tensor_dict[mn]=midi_to_pianoroll(PATH_PREFIX+mn)\n",
        "    index_array.append(i)\n",
        "    i=i+1\n",
        "#   return pd.DataFrame(\n",
        "#       data={DF_FEATURES: tensor_dict.values()}, \n",
        "#       index=tensor_dict.keys())\n",
        "  return tensor_dict\n",
        "\n",
        "################################################################################\n",
        "#                           Data Loading\n",
        "################################################################################\n",
        "\n",
        "df = pd.read_csv(\"MusicNST/midi_files/maestro-v1.0.0/maestro-v1.0.0.csv\", \n",
        "                 usecols=[DF_COMPOSER,DF_SPLIT,DF_FNAME])\n",
        "# unique_composers = df[DF_COMPOSER].unique()\n",
        "# NUM_UNIQUE_COMPOSERS = len(unique_composers)\n",
        "n=4\n",
        "unique_composers = df[DF_COMPOSER].value_counts()[:n].index.tolist()\n",
        "NUM_UNIQUE_COMPOSERS = len(unique_composers)\n",
        "\n",
        "df_comp = pd.DataFrame(data={\n",
        "    DF_COMPOSER_INDEX: range(len(unique_composers))},\n",
        "                      index=unique_composers)\n",
        "\n",
        "df = df.join(df_comp, on=DF_COMPOSER, how='inner')\n",
        "df = df[[DF_SPLIT, DF_FNAME, DF_COMPOSER_INDEX]]\n",
        "\n",
        "# Make test/train/validation split from our csv file will contain our golden\n",
        "# ie the composer and the midi_filename\n",
        "train_split = df.loc[df[DF_SPLIT]==\"train\"]\n",
        "train_split = train_split[[DF_COMPOSER_INDEX,DF_FNAME]]\n",
        "\n",
        "# Holds all of the train filenames\n",
        "train_filenames = train_split[DF_FNAME].tolist()\n",
        "random.shuffle(train_filenames)\n",
        "random.shuffle(train_filenames)\n",
        "\n",
        "validation_split = df.loc[df[DF_SPLIT]==\"validation\"]\n",
        "validation_split = validation_split[[DF_COMPOSER_INDEX,DF_FNAME]]\n",
        "\n",
        "test_split = df.loc[df[DF_SPLIT]==\"test\"]\n",
        "test_split = test_split[[DF_COMPOSER_INDEX,DF_FNAME]]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2U5_1ukZw7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@Title Model \n",
        "\n",
        "# def get_model(input_tensor):\n",
        "#   \"\"\"Creates and returns model object.\"\"\"\n",
        "#   ip = Input(tensor=tf.cast(tf.constant(input_tensor),dtype=tf.float32), \n",
        "#              batch_shape=input_tensor.shape)\n",
        "def get_model(shape):\n",
        "  ip = Input(batch_shape=shape)\n",
        "  layer_2 = Convolution1D(filters=256, kernel_size=128, activation='elu', padding='same')(ip)\n",
        "#   layer_2 = BatchNormalization(axis=-1)(layer_2)\n",
        "  layer_2 = Dropout(rate=0.1)(layer_2)\n",
        "  layer_2 = Convolution1D(filters=128, kernel_size=64, activation='elu', padding='same')(layer_2)\n",
        "  layer_2 = Dropout(rate=0.1)(layer_2)\n",
        "  layer_2 = Convolution1D(filters=64, kernel_size=32, activation='elu', padding='same')(layer_2)\n",
        "#   layer_2 = Convolution1D(filters=32, kernel_size=16, activation='elu', padding='same')(layer_2)\n",
        "#   layer_2 = Convolution1D(filters=16, kernel_size=8, activation='elu', padding='same')(layer_2)\n",
        "  layer_2 = Convolution1D(filters=1, kernel_size=4, activation='elu', padding='same')(layer_2)\n",
        "#   layer_2 = Reshape((BATCH_SIZE, MAX_DIM*4, 1))\n",
        "  layer_2 = Lambda(lambda x: backend.squeeze(x, axis=-1))(layer_2)\n",
        "  dense_1 = Dense(units=MAX_DIM, activation='elu')(layer_2)\n",
        "  dense_1 = Dense(units=MAX_DIM/2, activation='elu')(dense_1)\n",
        "  dense_1 = Dense(units=NUM_UNIQUE_COMPOSERS, activation='elu')(dense_1)\n",
        "  dense_1 = Dense(units=NUM_UNIQUE_COMPOSERS, activation='softmax')(dense_1)\n",
        "  return Model(ip, dense_1)\n",
        "\n",
        "def calculate_loss_per_batch_get_gradients(filenames, su):  \n",
        "  \"\"\"Calculates loss on a specified start_end index\"\"\"\n",
        "  midi_data_dict = load_midi_data_from_midi_files(filenames)\n",
        "  \n",
        "  file_to_use = pd.DataFrame(data={}, index=midi_data_dict.keys())\n",
        "  \n",
        "  labels_list=[]\n",
        "  tensors_list=[]\n",
        "  max_2_dim = 0\n",
        "  cut = 0\n",
        "  for key, value in midi_data_dict.items():\n",
        "    labels_list.append(su.loc[su[DF_FNAME]==key][DF_COMPOSER_INDEX].values[0])\n",
        "    max_2_dim = max(value.shape[0], max_2_dim)\n",
        "    if(value.shape[0] > MAX_DIM):\n",
        "      cut = cut + 1\n",
        "      value = value[:MAX_DIM,:]\n",
        "    elif(value.shape[0] < MAX_DIM):\n",
        "      value = np.pad(value, ((0,MAX_DIM - value.shape[0]),(0,0)), 'constant')\n",
        "    tensor = np.expand_dims(value, axis=0)\n",
        "    tensors_list.append(tensor)\n",
        "  print(\"Num cuts in batch %d\", cut)\n",
        "  # One hot labels \n",
        "  labels = np.eye(NUM_UNIQUE_COMPOSERS)[labels_list]\n",
        "  training_data=np.concatenate(tensors_list, axis=0)  \n",
        "  # Gotta pad first.\n",
        "#   training_data = np.concatenate(\n",
        "#       [np.pad(a , ((0,0), (0, max_2_dim-a.shape[1]), (0,0)), 'constant') \n",
        "#        for a in tensors_list], axis=0)\n",
        "  return training_data, labels\n",
        "#   model = get_model(training_data)\n",
        "  \n",
        "#   # Calculate the loss \n",
        "#   loss=losses.categorical_crossentropy(labels,model.output)\n",
        "#   grads = K.gradients(loss, model.output)\n",
        "#   outputs = [loss]\n",
        "#   if type(grads) in {list, tuple}:\n",
        "#       outputs += grads\n",
        "#   else:\n",
        "#       outputs.append(grads)  \n",
        "  return loss, model\n",
        "\n",
        "# calculate_loss_per_batch(train_filenames[0:3], train_split)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfVd_1cII_es",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3842
        },
        "outputId": "d4ec78a7-bf14-4309-f1ae-76d6380a78dc"
      },
      "source": [
        "#@Title Training Loop \n",
        "model_250 = get_model([BATCH_SIZE, MAX_DIM, 896])\n",
        "model_250.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "for j in range(3):\n",
        "  for i in range(NUM_ITER):\n",
        "    print(\"At iteration and epoch %d% , %d%\", i, j)\n",
        "    training_data, labels = calculate_loss_per_batch_get_gradients(train_filenames[i * BATCH_SIZE:(i+1) * BATCH_SIZE], train_split)\n",
        "  #   loss=losses.categorical_crossentropy(labels, model.output)\n",
        "  #   sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  #   model.compile(sgd, loss)\n",
        "    print(model_250.train_on_batch(training_data, labels))\n",
        "\n",
        "    # evaluate the model\n",
        "  #   scores = model.evaluate(training_data, labels)\n",
        "  #   print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    del training_data, labels\n",
        "  random.shuffle(train_filenames)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At iteration and epoch %d% , %d% 0 0\n",
            "Num cuts in batch %d 20\n",
            "[2.946462, 0.15]\n",
            "At iteration and epoch %d% , %d% 1 0\n",
            "Num cuts in batch %d 20\n",
            "[1.465884, 0.3]\n",
            "At iteration and epoch %d% , %d% 2 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4142858, 0.25]\n",
            "At iteration and epoch %d% , %d% 3 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4521363, 0.25]\n",
            "At iteration and epoch %d% , %d% 4 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3502523, 0.3]\n",
            "At iteration and epoch %d% , %d% 5 0\n",
            "Num cuts in batch %d 20\n",
            "[1.6555021, 0.1]\n",
            "At iteration and epoch %d% , %d% 6 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5060873, 0.3]\n",
            "At iteration and epoch %d% , %d% 7 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5817376, 0.1]\n",
            "At iteration and epoch %d% , %d% 8 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5891314, 0.3]\n",
            "At iteration and epoch %d% , %d% 9 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3966458, 0.15]\n",
            "At iteration and epoch %d% , %d% 10 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5074703, 0.3]\n",
            "At iteration and epoch %d% , %d% 11 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4623995, 0.05]\n",
            "At iteration and epoch %d% , %d% 12 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3260167, 0.4]\n",
            "At iteration and epoch %d% , %d% 13 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3915292, 0.3]\n",
            "At iteration and epoch %d% , %d% 14 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4148588, 0.25]\n",
            "At iteration and epoch %d% , %d% 15 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4145831, 0.35]\n",
            "At iteration and epoch %d% , %d% 16 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3652722, 0.4]\n",
            "At iteration and epoch %d% , %d% 17 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3161665, 0.45]\n",
            "At iteration and epoch %d% , %d% 18 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3713822, 0.35]\n",
            "At iteration and epoch %d% , %d% 19 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4025483, 0.35]\n",
            "At iteration and epoch %d% , %d% 20 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4327872, 0.25]\n",
            "At iteration and epoch %d% , %d% 21 0\n",
            "Num cuts in batch %d 20\n",
            "[1.309315, 0.45]\n",
            "At iteration and epoch %d% , %d% 22 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4012305, 0.25]\n",
            "At iteration and epoch %d% , %d% 23 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5507228, 0.15]\n",
            "At iteration and epoch %d% , %d% 24 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4510581, 0.15]\n",
            "At iteration and epoch %d% , %d% 0 1\n",
            "Num cuts in batch %d 20\n",
            "[1.4577905, 0.25]\n",
            "At iteration and epoch %d% , %d% 1 1\n",
            "Num cuts in batch %d 20\n",
            "[1.4790125, 0.2]\n",
            "At iteration and epoch %d% , %d% 2 1\n",
            "Num cuts in batch %d 20\n",
            "[1.4266452, 0.15]\n",
            "At iteration and epoch %d% , %d% 3 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3796428, 0.15]\n",
            "At iteration and epoch %d% , %d% 4 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3696756, 0.35]\n",
            "At iteration and epoch %d% , %d% 5 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3686727, 0.2]\n",
            "At iteration and epoch %d% , %d% 6 1\n",
            "Num cuts in batch %d 20\n",
            "[1.414725, 0.2]\n",
            "At iteration and epoch %d% , %d% 7 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3554449, 0.3]\n",
            "At iteration and epoch %d% , %d% 8 1\n",
            "Num cuts in batch %d 20\n",
            "[1.4452997, 0.15]\n",
            "At iteration and epoch %d% , %d% 9 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3193581, 0.5]\n",
            "At iteration and epoch %d% , %d% 10 1\n",
            "Num cuts in batch %d 20\n",
            "[1.4426767, 0.1]\n",
            "At iteration and epoch %d% , %d% 11 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3931842, 0.25]\n",
            "At iteration and epoch %d% , %d% 12 1\n",
            "Num cuts in batch %d 20\n",
            "[1.344126, 0.3]\n",
            "At iteration and epoch %d% , %d% 13 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3671467, 0.3]\n",
            "At iteration and epoch %d% , %d% 14 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3765093, 0.25]\n",
            "At iteration and epoch %d% , %d% 15 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3368499, 0.35]\n",
            "At iteration and epoch %d% , %d% 16 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3381398, 0.4]\n",
            "At iteration and epoch %d% , %d% 17 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3188007, 0.45]\n",
            "At iteration and epoch %d% , %d% 18 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3974835, 0.35]\n",
            "At iteration and epoch %d% , %d% 19 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3677162, 0.35]\n",
            "At iteration and epoch %d% , %d% 20 1\n",
            "Num cuts in batch %d 20\n",
            "[1.5144508, 0.25]\n",
            "At iteration and epoch %d% , %d% 21 1\n",
            "Num cuts in batch %d 20\n",
            "[1.3194987, 0.45]\n",
            "At iteration and epoch %d% , %d% 22 1\n",
            "Num cuts in batch %d 20\n",
            "[1.4441175, 0.25]\n",
            "At iteration and epoch %d% , %d% 23 1\n",
            "Num cuts in batch %d 20\n",
            "[1.4989451, 0.15]\n",
            "At iteration and epoch %d% , %d% 24 1\n",
            "Num cuts in batch %d 20\n",
            "[1.4848858, 0.15]\n",
            "At iteration and epoch %d% , %d% 0 2\n",
            "Num cuts in batch %d 20\n",
            "[1.4235022, 0.25]\n",
            "At iteration and epoch %d% , %d% 1 2\n",
            "Num cuts in batch %d 20\n",
            "[1.4394686, 0.2]\n",
            "At iteration and epoch %d% , %d% 2 2\n",
            "Num cuts in batch %d 20\n",
            "[1.4271809, 0.15]\n",
            "At iteration and epoch %d% , %d% 3 2\n",
            "Num cuts in batch %d 20\n",
            "[1.372055, 0.45]\n",
            "At iteration and epoch %d% , %d% 4 2\n",
            "Num cuts in batch %d 20\n",
            "[1.4037927, 0.25]\n",
            "At iteration and epoch %d% , %d% 5 2\n",
            "Num cuts in batch %d 20\n",
            "[1.3718011, 0.2]\n",
            "At iteration and epoch %d% , %d% 6 2\n",
            "Num cuts in batch %d 20\n",
            "[1.404075, 0.2]\n",
            "At iteration and epoch %d% , %d% 7 2\n",
            "Num cuts in batch %d 20\n",
            "[1.3621382, 0.3]\n",
            "At iteration and epoch %d% , %d% 8 2\n",
            "Num cuts in batch %d 20\n",
            "[1.4245381, 0.15]\n",
            "At iteration and epoch %d% , %d% 9 2\n",
            "Num cuts in batch %d 20\n",
            "[1.3304205, 0.5]\n",
            "At iteration and epoch %d% , %d% 10 2\n",
            "Num cuts in batch %d 20\n",
            "[1.4376314, 0.1]\n",
            "At iteration and epoch %d% , %d% 11 2\n",
            "Num cuts in batch %d 20\n",
            "[1.3944862, 0.25]\n",
            "At iteration and epoch %d% , %d% 12 2\n",
            "Num cuts in batch %d 20\n",
            "[1.3374331, 0.3]\n",
            "At iteration and epoch %d% , %d% 13 2\n",
            "Num cuts in batch %d 20\n",
            "[1.3558455, 0.3]\n",
            "At iteration and epoch %d% , %d% 14 2\n",
            "Num cuts in batch %d 20\n",
            "[1.3676023, 0.25]\n",
            "At iteration and epoch %d% , %d% 15 2\n",
            "Num cuts in batch %d 20\n",
            "[1.3215153, 0.35]\n",
            "At iteration and epoch %d% , %d% 16 2\n",
            "Num cuts in batch %d 20\n",
            "[1.3408374, 0.4]\n",
            "At iteration and epoch %d% , %d% 17 2\n",
            "Num cuts in batch %d 20\n",
            "[1.3223708, 0.45]\n",
            "At iteration and epoch %d% , %d% 18 2\n",
            "Num cuts in batch %d 20\n",
            "[1.4104204, 0.35]\n",
            "At iteration and epoch %d% , %d% 19 2\n",
            "Num cuts in batch %d 20\n",
            "[1.381978, 0.35]\n",
            "At iteration and epoch %d% , %d% 20 2\n",
            "Num cuts in batch %d 20\n",
            "[1.5167902, 0.25]\n",
            "At iteration and epoch %d% , %d% 21 2\n",
            "Num cuts in batch %d 20\n",
            "[1.3248932, 0.45]\n",
            "At iteration and epoch %d% , %d% 22 2\n",
            "Num cuts in batch %d 20\n",
            "[1.4258863, 0.25]\n",
            "At iteration and epoch %d% , %d% 23 2\n",
            "Num cuts in batch %d 20\n",
            "[1.5108228, 0.15]\n",
            "At iteration and epoch %d% , %d% 24 2\n",
            "Num cuts in batch %d 20\n",
            "[1.4751589, 0.15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2akWCDkrDmue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1772
        },
        "outputId": "dccd62bf-d4e6-4271-fc95-e5aa6327c35a"
      },
      "source": [
        "#@Title Training Loop \n",
        "model_one = get_model([BATCH_SIZE, MAX_DIM, 896])\n",
        "model_one.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "for j in range(3):\n",
        "  for i in range(NUM_ITER):\n",
        "    print(\"At iteration and epoch %d% , %d%\", i, j)\n",
        "    training_data, labels = calculate_loss_per_batch_get_gradients(train_filenames[i * BATCH_SIZE:(i+1) * BATCH_SIZE], train_split)\n",
        "  #   loss=losses.categorical_crossentropy(labels, model.output)\n",
        "  #   sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  #   model.compile(sgd, loss)\n",
        "    print(model_one.train_on_batch(training_data, labels))\n",
        "\n",
        "    # evaluate the model\n",
        "  #   scores = model.evaluate(training_data, labels)\n",
        "  #   print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    del training_data, labels"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At iteration and epoch %d% , %d% 0 0\n",
            "Num cuts in batch %d 20\n",
            "[1.6855037, 0.25]\n",
            "At iteration and epoch %d% , %d% 1 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5805612, 0.15]\n",
            "At iteration and epoch %d% , %d% 2 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4634602, 0.25]\n",
            "At iteration and epoch %d% , %d% 3 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4661157, 0.25]\n",
            "At iteration and epoch %d% , %d% 4 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4119016, 0.1]\n",
            "At iteration and epoch %d% , %d% 5 0\n",
            "Num cuts in batch %d 20\n",
            "[1.409792, 0.15]\n",
            "At iteration and epoch %d% , %d% 6 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5550728, 0.25]\n",
            "At iteration and epoch %d% , %d% 7 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4067249, 0.25]\n",
            "At iteration and epoch %d% , %d% 8 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4270213, 0.2]\n",
            "At iteration and epoch %d% , %d% 9 0\n",
            "Num cuts in batch %d 20\n",
            "[1.2976172, 0.55]\n",
            "At iteration and epoch %d% , %d% 10 0\n",
            "Num cuts in batch %d 20\n",
            "[1.374302, 0.4]\n",
            "At iteration and epoch %d% , %d% 11 0\n",
            "Num cuts in batch %d 20\n",
            "[1.51947, 0.15]\n",
            "At iteration and epoch %d% , %d% 12 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4293604, 0.15]\n",
            "At iteration and epoch %d% , %d% 13 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3989605, 0.3]\n",
            "At iteration and epoch %d% , %d% 14 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3322723, 0.4]\n",
            "At iteration and epoch %d% , %d% 15 0\n",
            "Num cuts in batch %d 20\n",
            "[1.276606, 0.5]\n",
            "At iteration and epoch %d% , %d% 16 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3385496, 0.35]\n",
            "At iteration and epoch %d% , %d% 17 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4182866, 0.25]\n",
            "At iteration and epoch %d% , %d% 18 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5000598, 0.1]\n",
            "At iteration and epoch %d% , %d% 19 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3949308, 0.4]\n",
            "At iteration and epoch %d% , %d% 20 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3506701, 0.35]\n",
            "At iteration and epoch %d% , %d% 21 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4634955, 0.2]\n",
            "At iteration and epoch %d% , %d% 22 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3627802, 0.3]\n",
            "At iteration and epoch %d% , %d% 23 0\n",
            "Num cuts in batch %d 20\n",
            "[1.2959867, 0.15]\n",
            "At iteration and epoch %d% , %d% 24 0\n",
            "Num cuts in batch %d 19\n",
            "[1.6216686, 0.2]\n",
            "At iteration and epoch %d% , %d% 25 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5226917, 0.15]\n",
            "At iteration and epoch %d% , %d% 26 0\n",
            "Num cuts in batch %d 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-656d3c139681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m#   sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m#   model.compile(sgd, loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [3,4] vs. [20,4]\n\t [[{{node training_1/Adam/gradients/loss_1/dense_4_loss/mul_grad/BroadcastGradientArgs}}]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBkq5DIXoGPC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1874
        },
        "outputId": "fbac4c5a-5cd1-4fc1-a4af-21515c631f65"
      },
      "source": [
        "#@Title Training Loop \n",
        "model = get_model([BATCH_SIZE, MAX_DIM, 896])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "for j in range(3):\n",
        "  for i in range(NUM_ITER):\n",
        "    print(\"At iteration and epoch %d% , %d%\", i, j)\n",
        "    training_data, labels = calculate_loss_per_batch_get_gradients(train_filenames[i * BATCH_SIZE:(i+1) * BATCH_SIZE], train_split)\n",
        "  #   loss=losses.categorical_crossentropy(labels, model.output)\n",
        "  #   sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  #   model.compile(sgd, loss)\n",
        "    print(model.train_on_batch(training_data, labels))\n",
        "\n",
        "    # evaluate the model\n",
        "  #   scores = model.evaluate(training_data, labels)\n",
        "  #   print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    del training_data, labels\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "At iteration and epoch %d% , %d% 0 0\n",
            "Num cuts in batch %d 20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "[2.3025174, 0.15]\n",
            "At iteration and epoch %d% , %d% 1 0\n",
            "Num cuts in batch %d 20\n",
            "[1.9169124, 0.3]\n",
            "At iteration and epoch %d% , %d% 2 0\n",
            "Num cuts in batch %d 20\n",
            "[1.7785218, 0.2]\n",
            "At iteration and epoch %d% , %d% 3 0\n",
            "Num cuts in batch %d 20\n",
            "[2.0761747, 0.2]\n",
            "At iteration and epoch %d% , %d% 4 0\n",
            "Num cuts in batch %d 19\n",
            "[1.8056618, 0.2]\n",
            "At iteration and epoch %d% , %d% 5 0\n",
            "Num cuts in batch %d 20\n",
            "[1.7365942, 0.1]\n",
            "At iteration and epoch %d% , %d% 6 0\n",
            "Num cuts in batch %d 20\n",
            "[1.403935, 0.25]\n",
            "At iteration and epoch %d% , %d% 7 0\n",
            "Num cuts in batch %d 19\n",
            "[1.717499, 0.15]\n",
            "At iteration and epoch %d% , %d% 8 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5800993, 0.25]\n",
            "At iteration and epoch %d% , %d% 9 0\n",
            "Num cuts in batch %d 20\n",
            "[1.6374344, 0.2]\n",
            "At iteration and epoch %d% , %d% 10 0\n",
            "Num cuts in batch %d 20\n",
            "[1.7233845, 0.2]\n",
            "At iteration and epoch %d% , %d% 11 0\n",
            "Num cuts in batch %d 19\n",
            "[1.6797113, 0.1]\n",
            "At iteration and epoch %d% , %d% 12 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4590657, 0.2]\n",
            "At iteration and epoch %d% , %d% 13 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4112613, 0.35]\n",
            "At iteration and epoch %d% , %d% 14 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5645254, 0.1]\n",
            "At iteration and epoch %d% , %d% 15 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5451331, 0.2]\n",
            "At iteration and epoch %d% , %d% 16 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4660703, 0.3]\n",
            "At iteration and epoch %d% , %d% 17 0\n",
            "Num cuts in batch %d 20\n",
            "[1.8473533, 0.25]\n",
            "At iteration and epoch %d% , %d% 18 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3589308, 0.35]\n",
            "At iteration and epoch %d% , %d% 19 0\n",
            "Num cuts in batch %d 18\n",
            "[1.5175844, 0.2]\n",
            "At iteration and epoch %d% , %d% 20 0\n",
            "Num cuts in batch %d 20\n",
            "[1.1840744, 0.55]\n",
            "At iteration and epoch %d% , %d% 21 0\n",
            "Num cuts in batch %d 19\n",
            "[1.6069933, 0.35]\n",
            "At iteration and epoch %d% , %d% 22 0\n",
            "Num cuts in batch %d 19\n",
            "[1.6408443, 0.2]\n",
            "At iteration and epoch %d% , %d% 23 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4833987, 0.3]\n",
            "At iteration and epoch %d% , %d% 24 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4416574, 0.2]\n",
            "At iteration and epoch %d% , %d% 25 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4410256, 0.25]\n",
            "At iteration and epoch %d% , %d% 26 0\n",
            "Num cuts in batch %d 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0190611937e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m#   sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m#   model.compile(sgd, loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [3,4] vs. [20,4]\n\t [[{{node training/Adam/gradients/loss/dense_2_loss/mul_grad/BroadcastGradientArgs}}]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCBhbjo6-8JP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1772
        },
        "outputId": "918baaed-2d49-4126-ff22-4ebede857d4d"
      },
      "source": [
        "for j in range(3):\n",
        "  for i in range(NUM_ITER):\n",
        "    print(\"At iteration and epoch %d% , %d%\", i, j)\n",
        "    training_data, labels = calculate_loss_per_batch_get_gradients(train_filenames[i * BATCH_SIZE:(i+1) * BATCH_SIZE], train_split)\n",
        "  #   loss=losses.categorical_crossentropy(labels, model.output)\n",
        "  #   sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  #   model.compile(sgd, loss)\n",
        "    print(model.train_on_batch(training_data, labels))\n",
        "\n",
        "    # evaluate the model\n",
        "  #   scores = model.evaluate(training_data, labels)\n",
        "  #   print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    del training_data, labels"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At iteration and epoch %d% , %d% 0 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4331439, 0.2]\n",
            "At iteration and epoch %d% , %d% 1 0\n",
            "Num cuts in batch %d 20\n",
            "[1.398399, 0.2]\n",
            "At iteration and epoch %d% , %d% 2 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4157279, 0.15]\n",
            "At iteration and epoch %d% , %d% 3 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4213254, 0.25]\n",
            "At iteration and epoch %d% , %d% 4 0\n",
            "Num cuts in batch %d 19\n",
            "[1.3730273, 0.35]\n",
            "At iteration and epoch %d% , %d% 5 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4121124, 0.3]\n",
            "At iteration and epoch %d% , %d% 6 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3319993, 0.4]\n",
            "At iteration and epoch %d% , %d% 7 0\n",
            "Num cuts in batch %d 19\n",
            "[1.369064, 0.4]\n",
            "At iteration and epoch %d% , %d% 8 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3323095, 0.4]\n",
            "At iteration and epoch %d% , %d% 9 0\n",
            "Num cuts in batch %d 20\n",
            "[1.458204, 0.25]\n",
            "At iteration and epoch %d% , %d% 10 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5126954, 0.25]\n",
            "At iteration and epoch %d% , %d% 11 0\n",
            "Num cuts in batch %d 19\n",
            "[1.4544828, 0.35]\n",
            "At iteration and epoch %d% , %d% 12 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4740227, 0.2]\n",
            "At iteration and epoch %d% , %d% 13 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3424504, 0.35]\n",
            "At iteration and epoch %d% , %d% 14 0\n",
            "Num cuts in batch %d 20\n",
            "[1.5195675, 0.1]\n",
            "At iteration and epoch %d% , %d% 15 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3277273, 0.25]\n",
            "At iteration and epoch %d% , %d% 16 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3056378, 0.45]\n",
            "At iteration and epoch %d% , %d% 17 0\n",
            "Num cuts in batch %d 20\n",
            "[1.607276, 0.35]\n",
            "At iteration and epoch %d% , %d% 18 0\n",
            "Num cuts in batch %d 20\n",
            "[1.3707519, 0.35]\n",
            "At iteration and epoch %d% , %d% 19 0\n",
            "Num cuts in batch %d 18\n",
            "[1.4855785, 0.2]\n",
            "At iteration and epoch %d% , %d% 20 0\n",
            "Num cuts in batch %d 20\n",
            "[1.177327, 0.55]\n",
            "At iteration and epoch %d% , %d% 21 0\n",
            "Num cuts in batch %d 19\n",
            "[1.4744139, 0.3]\n",
            "At iteration and epoch %d% , %d% 22 0\n",
            "Num cuts in batch %d 19\n",
            "[1.4948517, 0.2]\n",
            "At iteration and epoch %d% , %d% 23 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4893191, 0.15]\n",
            "At iteration and epoch %d% , %d% 24 0\n",
            "Num cuts in batch %d 20\n",
            "[1.447161, 0.2]\n",
            "At iteration and epoch %d% , %d% 25 0\n",
            "Num cuts in batch %d 20\n",
            "[1.4003947, 0.25]\n",
            "At iteration and epoch %d% , %d% 26 0\n",
            "Num cuts in batch %d 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-965a58341517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m#   sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m#   model.compile(sgd, loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [3,4] vs. [20,4]\n\t [[{{node training/Adam/gradients/loss/dense_2_loss/mul_grad/BroadcastGradientArgs}}]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGjyeyu05a3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@Title Training Loop \n",
        "model = get_model([BATCH_SIZE, MAX_DIM, 896])\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "for i in range(NUM_ITER):\n",
        "  print(\"At iteration %d\", i)\n",
        "  training_data, labels = calculate_loss_per_batch_get_gradients(train_filenames[i * BATCH_SIZE:(i+1) * BATCH_SIZE], train_split)\n",
        "#   loss=losses.categorical_crossentropy(labels, model.output)\n",
        "#   sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "#   model.compile(sgd, loss)\n",
        "  print(model.train_on_batch(training_data, labels))\n",
        "  \n",
        "  # evaluate the model\n",
        "  scores = model.evaluate(training_data, labels)\n",
        "  print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  del training_data, labels"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}