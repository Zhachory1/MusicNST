{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestingMusicVAE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhachory1/MusicNST/blob/master/TestingMusicVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vp1CG471zGqx",
        "outputId": "627ccb66-d6cf-453a-8204-3823c12889d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "#@title Install Dependencies\n",
        "print 'Installing dependencies...'\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -q pyfluidsynth\n",
        "!pip install py-midi pretty_midi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n",
            "Requirement already satisfied: py-midi in /usr/local/lib/python2.7/dist-packages (1.2.5)\n",
            "Requirement already satisfied: pretty_midi in /usr/local/lib/python2.7/dist-packages (0.2.8)\n",
            "Requirement already satisfied: pyserial in /usr/local/lib/python2.7/dist-packages (from py-midi) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from pretty_midi) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python2.7/dist-packages (from pretty_midi) (1.16.3)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python2.7/dist-packages (from pretty_midi) (1.2.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkFJbuvpmcdI",
        "colab_type": "code",
        "outputId": "4c04774e-1388-4fbd-b998-f7ac4fc24801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@title Import Data from GitHub \n",
        "!cd MusicNST/ && git pull\n",
        "!git clone https://github.com/Zhachory1/MusicNST.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n",
            "fatal: destination path 'MusicNST' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyxJrC9woeAD",
        "colab_type": "code",
        "outputId": "3a89ef77-f94b-49cb-a220-945e118b04f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "#@Title Imports\n",
        "import os\n",
        "import copy\n",
        "import fnmatch\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import warnings\n",
        "import midi\n",
        "import pretty_midi\n",
        "import pandas as pd\n",
        "import collections\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Note sequence to piano roll\n",
        "from magenta.music.pianoroll_lib import PianorollSequence\n",
        "import magenta.music.sequences_lib as seq_lib\n",
        "import magenta.music as mm\n",
        "import magenta.models.music_vae.data as data\n",
        "from magenta.protobuf import music_pb2\n",
        "\n",
        "from keras import optimizers, losses\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense, Lambda, LSTM, RepeatVector, Bidirectional\n",
        "from keras import backend as K                   \n",
        "from keras.utils import Sequence\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "def play(note_sequence):\n",
        "  mm.play_sequence(note_sequence, synth=mm.fluidsynth)\n",
        "\n",
        "def download(note_sequence, filename):\n",
        "  mm.sequence_proto_to_midi_file(note_sequence, filename)\n",
        "  files.download(filename)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 3298728482037980355\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 655702652170079239\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 13315137474284640180\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 12721733632\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 7952987640063017278\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSJ8LplgHhQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@Title Helper functions and global variables \n",
        "\n",
        "################################################################################\n",
        "#                                 Constants \n",
        "################################################################################\n",
        "DF_FNAME = 'midi_filename'\n",
        "DF_FEATURES = 'midi_features'\n",
        "DF_SPLIT = \"split\"\n",
        "PATH_PREFIX = './MusicNST/midi_files/maestro-v1.0.0/'\n",
        "INDEX_PATH = PATH_PREFIX + \"maestro-v1.0.0.csv\"\n",
        "################################################################################\n",
        "#                    Helper Functions to manipulate input data. \n",
        "################################################################################\n",
        "\n",
        "def new_note_sequence(steps_per_quarter, step_per_split):\n",
        "  \"\"\"Helper function to quickly create default NoteSequence proto\"\"\"\n",
        "  qns = music_pb2.NoteSequence()\n",
        "  qns.quantization_info.steps_per_quarter = steps_per_quarter\n",
        "  qns.total_quantized_steps = step_per_split\n",
        "  qns.total_time = 2 * int(step_per_split/steps_per_quarter)\n",
        "  time_signature = qns.time_signatures.add()\n",
        "  time_signature.numerator = 4\n",
        "  time_signature.denominator = 4\n",
        "  time_signature.time = 0\n",
        "  tempo = qns.tempos.add()\n",
        "  tempo.qpm = 120 # quarters per minute\n",
        "  tempo.time = 0\n",
        "  return qns\n",
        "  \n",
        "\n",
        "def split_note_sequences(note_sequence, bars_per_split=2, steps_per_quarter=4):\n",
        "  \"\"\"Splits note_sequences into new note sequence of length bars_per_split\n",
        "  \n",
        "  Given a note_sequence, we quantize it to steps_per_quarter notes in a beat. We\n",
        "  then grab 16*bar_per_split steps into separate note_sequences, assuming that \n",
        "  the bars are in 4|4 time. Setting bars_per_split to 2 will return an array of \n",
        "  note sequences that are of length 32. \n",
        "  \n",
        "  Args:\n",
        "    note_sequence:      NoteSequence, input note sequence to split\n",
        "    bars_per_split:     int, number of bars each output note sequence will have. \n",
        "    steps_per_quarter:  int, number of steps in each quarter note.\n",
        "  Return\n",
        "    np.array[NoteSequences] - output of note sequences of length \n",
        "                              bars_per_split*16\n",
        "  \"\"\"\n",
        "  quantized_seq = mm.quantize_note_sequence(note_sequence, steps_per_quarter)\n",
        "  # Copy QNS into it's own note_sequences, and clear out its notes. We \n",
        "  # are going to iterate over the notes until we hit split boundaries. \n",
        "  # Once we hit a boundary, we split any ongoing notes in two, one for\n",
        "  # the new QNS we have been collecting and one for the new one we will \n",
        "  # make. Once the bar is completed, we will createa PianorollSequence\n",
        "  # from it, and start on the next bar. \n",
        "  step_per_split = int(steps_per_quarter*4) * bars_per_split\n",
        "  split_steps = range(0, quantized_seq.total_quantized_steps, step_per_split)\n",
        "  current_split = 0\n",
        "  steps_per_quarter = quantized_seq.quantization_info.steps_per_quarter \n",
        "  qns = new_note_sequence(steps_per_quarter, step_per_split)\n",
        "  pianoroll_seqs = []\n",
        "  extended_notes = new_note_sequence(steps_per_quarter, step_per_split)\n",
        "  for note in quantized_seq.notes:\n",
        "    if note.quantized_start_step >= split_steps[current_split] + step_per_split: \n",
        "      # End of split sections. Close up QNS and set up next split.\n",
        "      pianoroll_seqs.append(copy.deepcopy(qns))\n",
        "      qns = extended_notes\n",
        "      extended_notes = new_note_sequence(steps_per_quarter, step_per_split)\n",
        "      current_split += 1\n",
        "\n",
        "    if note.quantized_end_step >= split_steps[current_split] + step_per_split:\n",
        "      # Note extends past this split. Split the note into two notes. Add note\n",
        "      # from note.start_step to end of current split. Save note from start of \n",
        "      # next split to note.end_step\n",
        "      first_half = qns.notes.add()\n",
        "      first_half.pitch = note.pitch\n",
        "      first_half.quantized_start_step = note.quantized_start_step - split_steps[current_split] \n",
        "      first_half.quantized_end_step = step_per_split\n",
        "\n",
        "      second_half = extended_notes.notes.add()\n",
        "      second_half.pitch = note.pitch\n",
        "      second_half.quantized_start_step = 0\n",
        "      second_half.quantized_end_step = note.quantized_end_step - split_steps[current_split] \n",
        "    else:\n",
        "      # This is the normal route, where we just copy the note from the\n",
        "      # sequence into the new one.\n",
        "      new_note = qns.notes.add()\n",
        "      new_note.pitch = note.pitch\n",
        "      new_note.quantized_start_step = note.quantized_start_step - split_steps[current_split] \n",
        "      new_note.quantized_end_step = note.quantized_end_step - split_steps[current_split] \n",
        "  pianoroll_seqs.append(copy.deepcopy(qns))\n",
        "\n",
        "  return pianoroll_seqs\n",
        "\n",
        "def midi_to_pianoroll(midi_filename, play_track=False):\n",
        "  \"\"\"Converts Midi Files into np.arrays and concats them.\n",
        "  \n",
        "  The arrays have the following types:\n",
        "  active, weights, onsets, onset_velocities, active_velocities, offsets, \n",
        "  control_changes.\n",
        "  \n",
        "  However we are only pulling the note.onsets.\n",
        "  \n",
        "  Args:\n",
        "    midi_filename: string, filename\n",
        "  Returns: \n",
        "    np.array - train tensor (33, 128, 1)\n",
        "  \"\"\"\n",
        "  midi_file = pretty_midi.PrettyMIDI(midi_filename)\n",
        "  note_seq = mm.midi_to_sequence_proto(midi_file)\n",
        "  if play_track:\n",
        "    play(note_seq)\n",
        "  split_note_seqs = split_note_sequences(note_seq)\n",
        "  final_array = []\n",
        "  for note_seq in split_note_seqs:\n",
        "    # This outputs (33, 128, 1) It's 33 instead of 32 because it adds an end \n",
        "    # token at the end of every sequence.\n",
        "    pnt = seq_lib.sequence_to_pianoroll(\n",
        "        note_seq, 2, data.MIN_MIDI_PITCH, data.MAX_MIDI_PITCH)\n",
        "    t_list = [pnt.onsets]\n",
        "    final_array.append(np.concatenate(t_list, axis=-1))\n",
        "  return np.array(final_array)\n",
        "\n",
        "def pianoroll_to_notes(pianoroll, opt_midi_file_name=\"\"):\n",
        "  \"\"\"Helper to obtain note_seq.\"\"\"\n",
        "  event_list = PianorollSequence(steps_per_quarter=4)\n",
        "  for i, e in enumerate(pianoroll):\n",
        "    event_list.append(frozenset(np.where(e)[0]))\n",
        "  note_seq = event_list.to_sequence()\n",
        "  if opt_midi_file_name != \"\":\n",
        "    download(note_seq, opt_midi_file_name)\n",
        "  return note_seq\n",
        "\n",
        "def load_midi_data_from_midi_files(filenames):\n",
        "  \"\"\"Loads files.\n",
        "  Args:\n",
        "    filenames: list of strings, midi filenames\n",
        "  Returns: \n",
        "    pandas dataframe.\"\"\"\n",
        "  tensor_dict = collections.OrderedDict()\n",
        "  index_array = []\n",
        "  i = 0\n",
        "  for mn in filenames:\n",
        "    tensor_dict[mn]=midi_to_pianoroll(PATH_PREFIX+mn)\n",
        "    index_array.append(i)\n",
        "    i=i+1\n",
        "  return tensor_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCFu6m-nDbfK",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "f1f619db-85ca-4477-fa45-a3b777df9485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@Title Testing reading in files and converting to piano roll\n",
        "df = pd.read_csv(\"MusicNST/midi_files/maestro-v1.0.0/maestro-v1.0.0.csv\", \n",
        "                 usecols=[DF_SPLIT,DF_FNAME])\n",
        "\n",
        "midi_filename = df.loc[0].midi_filename\n",
        "print(midi_filename)\n",
        "midi = load_midi_data_from_midi_files([midi_filename])\n",
        "midi_data = np.array(midi[midi_filename])\n",
        "midi_data.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2017/MIDI-Unprocessed_066_PIANO066_MID--AUDIO-split_07-07-17_Piano-e_3-02_wav--3.midi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(115, 33, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PndiRxEl4YQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MusicVaeDataGenerator(Sequence):\n",
        "    \"\"\"Generates data for Keras from MIDI files\"\"\"\n",
        "    def __init__(self, list_IDs, batch_size=1, dim=(33, 896),\n",
        "                 shuffle=True, bars_per_split=2):\n",
        "        \"\"\"Initialization\"\"\"\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size # Number of FILES to read in at one time\n",
        "        self.list_IDs = list_IDs\n",
        "        self.shuffle = shuffle\n",
        "        self.bars_per_split = bars_per_split\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data\"\"\"\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        \"\"\"Generates data containing batch_size samples\"\"\" # X : (n_samples, *dim)\n",
        "        # Initialization\n",
        "        X = []\n",
        "        #y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        loaded_data = load_midi_data_from_midi_files(list_IDs_temp)\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample. We grab many samples from one midi. So we concat \n",
        "            # everything in the batch dimension. Even though we have a batch \n",
        "            # size of eight, we mean we are reading in 8 files. We split the \n",
        "            # midi at 2 bar increments, then each of split split will be apart \n",
        "            # of the batch.   \n",
        "            X.append(loaded_data[ID]) \n",
        "\n",
        "            # If we wanted to store a labe, we would do it here.\n",
        "            #y[i] = self.labels[ID]\n",
        "        X = np.concatenate(X, axis=0)\n",
        "        \n",
        "        \n",
        "        return X, X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI_eLWlyQI4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@Title Make data generators\n",
        "df = pd.read_csv(\"MusicNST/midi_files/maestro-v1.0.0/maestro-v1.0.0.csv\", \n",
        "                 usecols=[DF_SPLIT,DF_FNAME])\n",
        "\n",
        "\n",
        "\n",
        "# Make test/train/validation split from our csv file will contain our golden\n",
        "# ie the composer and the midi_filename\n",
        "train_split = df.loc[df[DF_SPLIT]==\"train\"]\n",
        "train_filenames = train_split[DF_FNAME].tolist()\n",
        "train_generator = MusicVaeDataGenerator(train_filenames)\n",
        "\n",
        "\n",
        "validation_split = df.loc[df[DF_SPLIT]==\"validation\"]\n",
        "validation_filenames = validation_split[DF_FNAME].tolist()\n",
        "validation_generator = MusicVaeDataGenerator(validation_filenames)\n",
        "\n",
        "test_split = df.loc[df[DF_SPLIT]==\"test\"]\n",
        "test_filenames = test_split[DF_FNAME].tolist()\n",
        "test_generator = MusicVaeDataGenerator(test_filenames)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJWKFnqfS6x3",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "84095442-827b-41cd-ed95-e015463556c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Test data generators\n",
        "batch, _ = train_generator.__getitem__(0)\n",
        "batch.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(125, 33, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB2Pf6QYTOen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@Title Define the VAE\n",
        "def Encoder(x, input_dim, hidden_dim, latent_dim, epsilon_std):\n",
        "    # LSTM encoding\n",
        "    h = Bidirectional(LSTM(hidden_dim,  return_sequences=True, name=\"\"))(x)\n",
        "    h = Bidirectional(LSTM(hidden_dim, ))(h)\n",
        "    \n",
        "\n",
        "    # VAE Z layer\n",
        "    z_mean = Dense(latent_dim)(h)\n",
        "    z_log_sigma = Dense(latent_dim)(h)\n",
        "    \n",
        "    def sampling(args):\n",
        "        z_mean, z_log_sigma = args\n",
        "        # Note that this batch size DOES NOT correlate with the batch size in \n",
        "        # the data generator classes.\n",
        "        def grab_normal(unused_arg):\n",
        "          return K.random_normal(shape=(latent_dim,), mean=0., stddev=epsilon_std)\n",
        "        epsilon = K.map_fn(grab_normal, z_log_sigma, dtype=tf.float32)\n",
        "\n",
        "        return z_mean + z_log_sigma * epsilon\n",
        "\n",
        "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "    # so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
        "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
        "    \n",
        "    return Model(x, z_mean), z_mean, z_log_sigma, z  \n",
        "\n",
        "def Decoder(z, timesteps, input_dim, hidden_dim, latent_dim):\n",
        "    # Making reusable layers for botht the generator model and the VAE model\n",
        "    decoder_h_1 = LSTM(hidden_dim, return_sequences=True)\n",
        "    decoder_h_2 = LSTM(hidden_dim, return_sequences=True)\n",
        "    decoder_mean = LSTM(input_dim, return_sequences=True)\n",
        "\n",
        "    # VAE model\n",
        "    h_decoded = RepeatVector(timesteps)(z)\n",
        "    h_decoded = decoder_h_1(h_decoded)\n",
        "    h_decoded = decoder_h_2(h_decoded)\n",
        "    x_decoded_mean = decoder_mean(h_decoded)\n",
        "    \n",
        "    # generator, from latent space to reconstructed inputs\n",
        "    decoder_input = Input(shape=(latent_dim,))\n",
        "    _h_decoded = RepeatVector(timesteps)(decoder_input)\n",
        "    _h_decoded = decoder_h_1(_h_decoded)\n",
        "    _h_decoded = decoder_h_2(_h_decoded)\n",
        "    _x_decoded_mean = decoder_mean(_h_decoded)\n",
        "    decoder = Model(decoder_input, _x_decoded_mean)\n",
        "    \n",
        "    # return generator model and outputs for VAE to use\n",
        "    return decoder, x_decoded_mean\n",
        "    \n",
        "def VAE(input_dim, \n",
        "        timesteps,  \n",
        "        hidden_dim, \n",
        "        latent_dim,\n",
        "        epsilon_std=1.):\n",
        "    # Create 3 models, encoder, decoder, and the full VAE. They all share the \n",
        "    # same weights so I can use them all. \n",
        "    \n",
        "    # Creating input everyone can use.\n",
        "    x = Input(shape=(timesteps, input_dim,))\n",
        "    \n",
        "    # Pull encoder and decoder with variables we need.\n",
        "    encoder, z_mean, z_log_sigma, z = Encoder(x, input_dim, hidden_dim, latent_dim, epsilon_std)\n",
        "    decoder, x_decoded_mean = Decoder(z, timesteps, input_dim, hidden_dim, latent_dim)\n",
        "    \n",
        "    # Make VAE model\n",
        "    vae = Model(x, x_decoded_mean)\n",
        "    \n",
        "    # Describe the loss function\n",
        "    def vae_loss(x, x_decoded_mean):\n",
        "        # Loss between x and reconstructed x.\n",
        "        xent_loss = losses.binary_crossentropy(x, x_decoded_mean)\n",
        "        # Divergence between z and a guassian function. \n",
        "        kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
        "        loss = xent_loss + 0.1 * kl_loss\n",
        "        return loss\n",
        "\n",
        "    vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
        "  \n",
        "    return vae, encoder, decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iP0MMx6Z0Hf",
        "colab_type": "code",
        "outputId": "50a62a64-293c-45b0-cc01-4697b5b647ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "model, enc, gen = VAE(input_dim=128, \n",
        "        timesteps=33, \n",
        "        hidden_dim=64,\n",
        "        latent_dim=64,\n",
        "        epsilon_std=1.)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iOa7n2s1JKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# files.upload()\n",
        "# model.load_weights(\"model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZeTmqJwTgwp",
        "colab_type": "code",
        "outputId": "e7178f61-bf43-4305-f53d-0c033e0b207a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "#@title Train model on dataset\n",
        "model.fit_generator(generator=train_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    use_multiprocessing=True,\n",
        "                    epochs=10,\n",
        "                    workers=8)\n",
        "\n",
        "print model.evaluate_generator(generator=test_generator,\n",
        "                         use_multiprocessing=True,\n",
        "                         workers=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n",
            "954/954 [==============================] - 2543s 3s/step - loss: 0.0200 - val_loss: 0.0164\n",
            "Epoch 2/10\n",
            "954/954 [==============================] - 798s 837ms/step - loss: 0.0166 - val_loss: 0.0157\n",
            "Epoch 3/10\n",
            "954/954 [==============================] - 769s 806ms/step - loss: 0.0156 - val_loss: 0.0147\n",
            "Epoch 4/10\n",
            "954/954 [==============================] - 777s 815ms/step - loss: 0.0153 - val_loss: 0.0147\n",
            "Epoch 5/10\n",
            "954/954 [==============================] - 774s 812ms/step - loss: 0.0148 - val_loss: 0.0137\n",
            "Epoch 6/10\n",
            "954/954 [==============================] - 769s 806ms/step - loss: 0.0142 - val_loss: 0.0132\n",
            "Epoch 7/10\n",
            "954/954 [==============================] - 775s 813ms/step - loss: 0.0139 - val_loss: 0.0131\n",
            "Epoch 8/10\n",
            "954/954 [==============================] - 772s 809ms/step - loss: 0.0136 - val_loss: 0.0123\n",
            "Epoch 9/10\n",
            "255/954 [=======>......................] - ETA: 9:02 - loss: 0.0130"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR1jx_Ufnf3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"MusicNST/midi_files/maestro-v1.0.0/maestro-v1.0.0.csv\", \n",
        "                 usecols=[DF_SPLIT,DF_FNAME])\n",
        "midi_filename = df.loc[0].midi_filename\n",
        "midi = load_midi_data_from_midi_files([midi_filename])\n",
        "midi_data = np.array(midi[midi_filename])\n",
        "midi_data.shape\n",
        "pred = model.predict(midi_data)\n",
        "pred = np.concatenate([pr for pr in pred], axis=0)\n",
        "threshold = 0.2\n",
        "new_pianoroll = []\n",
        "for step in pred:\n",
        "  if np.where(step >= threshold, 1, 0).sum() > 6:\n",
        "    top_k_notes = step.argsort()[-6:][::-1]\n",
        "    new_step = np.zeros_like(step)\n",
        "    new_step[top_k_notes] = 1\n",
        "    new_pianoroll.append(new_step)\n",
        "  else:\n",
        "    new_pianoroll.append(step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x98z77WDclM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x, axis=-1):\n",
        "    y = np.exp(x - np.max(x, axis, keepdims=True))\n",
        "    return y / np.sum(y, axis, keepdims=True)\n",
        " \n",
        "print(new_pianoroll[0])\n",
        "print(midi_data[0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm_IprX6EUGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threshold = 0.2\n",
        "\n",
        "def encode_midi(midi_file, encoder):\n",
        "    midi_data = midi_to_pianoroll(midi_file, False)\n",
        "    return encoder.predict_on_batch(midi_data)\n",
        "\n",
        "def decode_piano(latent_code, decoder, opt_midi_file_name=\"\"):\n",
        "    pianoroll = decoder.predict_on_batch(latent_code)\n",
        "    pianoroll = np.concatenate([pr for pr in pianoroll], axis=0)\n",
        "    pianoroll[pianoroll < threshold] = 0\n",
        "    # Either make all or up to 6 notes play at the same step.\n",
        "    new_pianoroll = []\n",
        "    for step in pianoroll:\n",
        "      if np.where(step >= threshold, 1, 0).sum() > 6:\n",
        "        top_k_notes = step.argsort()[-6:][::-1]\n",
        "        new_step = np.zeros_like(step)\n",
        "        new_step[top_k_notes] = 1\n",
        "        new_pianoroll.append(new_step)\n",
        "      else:\n",
        "        new_pianoroll.append(step)\n",
        "    return pianoroll_to_notes(np.array(pianoroll), opt_midi_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzIPiDl3ryLy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "c3ff5e03-114a-4bb5-84cf-db0243eb79ea"
      },
      "source": [
        "latent_code = encode_midi(PATH_PREFIX+midi_filename, enc)\n",
        "print(latent_code.mean(axis=1))\n",
        "ns_reconstructed = decode_piano(latent_code, gen, '')\n",
        "# ns_reconstructed\n",
        "play(ns_reconstructed)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.00223478  0.00074569 -0.00099466 -0.00238259 -0.00146582  0.0012129\n",
            "  0.000951    0.00149064 -0.00010034 -0.00045232  0.00101645  0.00067532\n",
            " -0.00209925 -0.00126786 -0.00231596 -0.00019875 -0.00040246 -0.00092321\n",
            "  0.00026902  0.00100347 -0.00060387 -0.00133428 -0.00039968 -0.00123715\n",
            " -0.00145964 -0.00163855 -0.00284389 -0.00101732  0.00124079  0.00149599\n",
            "  0.00137052  0.00021626 -0.00070003 -0.00088385  0.00070703 -0.00138334\n",
            " -0.00109035  0.00117931  0.0009407   0.00011232 -0.00150819 -0.0025395\n",
            " -0.002242   -0.00120201 -0.00123377 -0.00196918 -0.00129294 -0.00135233\n",
            "  0.00054151 -0.00162069 -0.00114228 -0.00098012 -0.00190167 -0.00127972\n",
            " -0.0006112  -0.00180615 -0.00109066 -0.00176481  0.00217141  0.00044889\n",
            "  0.00121451 -0.00052481 -0.00155938 -0.00133876 -0.00249004 -0.00148853\n",
            " -0.00075242  0.00213555  0.00188526 -0.00066146 -0.00013513 -0.00193033\n",
            " -0.00265688 -0.00175924 -0.00105177 -0.00157387 -0.00177841 -0.00170079\n",
            " -0.00142943  0.00119019  0.00162095 -0.00062805 -0.00169294 -0.00030189\n",
            " -0.00125981  0.00035057  0.00063467  0.00025884 -0.00020322 -0.00170897\n",
            " -0.00260711 -0.00160329 -0.00161688 -0.00220294 -0.00273623 -0.00261533\n",
            " -0.00193539 -0.00019132  0.00136648  0.00100738  0.00201457  0.00149599\n",
            "  0.00100766  0.00120125 -0.00073102  0.00131472  0.00130701  0.00061809\n",
            "  0.00061992  0.00061295  0.00096693  0.00123423 -0.00086336 -0.00109408\n",
            " -0.00081106]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div id=\"id_1\"> </div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFmxyIrbuTSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"model_bilstm_weights.h5\")\n",
        "enc.save_weights(\"encoder_bilstm_weights.h5\")\n",
        "gen.save_weights(\"decoder_bilstm_weights.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ-pRahL0L-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(\"model_bilstm_weights.h5\")\n",
        "files.download(\"encoder_bilstm_weights.h5\")\n",
        "files.download(\"decoder_bilstm_weights.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0i52rk52I-F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdc18019-379a-4e41-9b50-038f9618baed"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decoder_bilstm.h5  encoder_bilstm.h5  model_bilstm.h5  MusicNST  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl1gne4Sif1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usIBcgBPixAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92pn3VGMiykK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}