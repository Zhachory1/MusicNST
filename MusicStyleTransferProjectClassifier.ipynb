{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MusicStyleTransferProject.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhachory1/MusicNST/blob/master/MusicStyleTransferProjectClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4a4OSs6VNnb",
        "colab_type": "code",
        "outputId": "c0867032-4bc6-4fc8-eda8-42f5edb0a659",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "#@title Install Dependencies\n",
        "print 'Installing dependencies...'\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -q pyfluidsynth\n",
        "!pip install py-midi pretty_midi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n",
            "Requirement already satisfied: py-midi in /usr/local/lib/python2.7/dist-packages (1.2.5)\n",
            "Requirement already satisfied: pretty_midi in /usr/local/lib/python2.7/dist-packages (0.2.8)\n",
            "Requirement already satisfied: pyserial in /usr/local/lib/python2.7/dist-packages (from py-midi) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from pretty_midi) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python2.7/dist-packages (from pretty_midi) (1.16.3)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python2.7/dist-packages (from pretty_midi) (1.2.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_JvO29YNynR",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "58e5d369-fab3-4727-994a-e710010e7bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "##@title Import Libraries\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import warnings\n",
        "import midi\n",
        "import pretty_midi\n",
        "\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "from scipy.misc import imread, imresize, imsave, fromimage, toimage\n",
        "\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers.convolutional import Convolution2D, AveragePooling2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils.layer_utils import convert_all_kernels_in_model  \n",
        "\n",
        "#@Title Imports\n",
        "import os\n",
        "import copy\n",
        "import fnmatch\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import warnings\n",
        "import midi\n",
        "import pretty_midi\n",
        "import pandas as pd\n",
        "import collections\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Note sequence to piano roll\n",
        "from magenta.music.pianoroll_lib import PianorollSequence\n",
        "import magenta.music.sequences_lib as seq_lib\n",
        "import magenta.music as mm\n",
        "from magenta.music.midi_io import note_sequence_to_pretty_midi\n",
        "import magenta.models.music_vae.data as data\n",
        "from magenta.protobuf import music_pb2\n",
        "\n",
        "from keras import optimizers, losses\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense, Lambda, LSTM, RepeatVector\n",
        "from keras import backend as K\n",
        "from keras.utils import Sequence\n",
        "from keras import optimizers, objectives\n",
        "from keras import backend, losses, utils\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense, Lambda, LSTM, RepeatVector\n",
        "from keras.layers.convolutional import Convolution2D, AveragePooling2D, MaxPooling2D, Convolution1D\n",
        "from keras import backend as K\n",
        "from keras.utils import Sequence, to_categorical\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils.layer_utils import convert_all_kernels_in_model  \n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "def play(note_sequence):\n",
        "  mm.play_sequence(note_sequence, synth=mm.fluidsynth)\n",
        "\n",
        "def download(note_sequence, filename):\n",
        "  mm.sequence_proto_to_midi_file(note_sequence, filename)\n",
        "  files.download(filename)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 4483984266628286195\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 2991451838012244064\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 14655156651526091643\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 6412137268\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 13162901454224045114\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VEQTenNUxP-",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load VGG Model \n",
        "TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "TF_19_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7SMwAToVfgA",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Global Params\n",
        "result_prefix = \"\"\n",
        "style_masks = None\n",
        "content_mask = None\n",
        "color_mask = None\n",
        "content_weight = 0.025\n",
        "style_weight = [1.0]\n",
        "style_scale =1.0\n",
        "total_variation_weight = 8.5e-5\n",
        "num_iter = 100\n",
        "model = \"vgg16\"\n",
        "content_loss_type = 0\n",
        "rescale_image = \"False\"\n",
        "rescale_method =\"bilinear\"\n",
        "content_layer =\"dense1\"\n",
        "init_image = \"content\"\n",
        "pooltype = \"max\"\n",
        "preserve_color = \"False\"\n",
        "min_improvement = 0.0\n",
        "pooltype = 1 if pooltype == \"ave\" else 0\n",
        "# dimensions of the generated picture.\n",
        "input_width = input_height = 0\n",
        "input_WIDTH = input_HEIGHT = 0\n",
        "aspect_ratio = 2\n",
        "\n",
        "music_size = 128\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3FmCx5fIPf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@Title Helper functions and global variables \n",
        "\n",
        "################################################################################\n",
        "#                                 Constants \n",
        "################################################################################\n",
        "BATCH_SIZE = 50\n",
        "NUM_ITER = 25\n",
        "DF_FNAME = 'midi_filename'\n",
        "DF_FEATURES = 'midi_features'\n",
        "DF_COMPOSER = 'canonical_composer'\n",
        "DF_COMPOSER_INDEX = 'composer_idx'\n",
        "DF_SPLIT = \"split\"\n",
        "PATH_PREFIX = './MusicNST/midi_files/maestro-v1.0.0/'\n",
        "INDEX_PATH = PATH_PREFIX + \"maestro-v1.0.0.csv\"\n",
        "MAX_DIM = 500\n",
        "COUNT_CUTTING = 0\n",
        "NUM_UNIQUE_COMPOSERS = 4\n",
        "################################################################################\n",
        "#                    Helper Functions to manipulate input data. \n",
        "################################################################################\n",
        "\n",
        "def new_note_sequence(steps_per_quarter, step_per_split):\n",
        "  \"\"\"Helper function to quickly create default NoteSequence proto\"\"\"\n",
        "  qns = music_pb2.NoteSequence()\n",
        "  qns.quantization_info.steps_per_quarter = steps_per_quarter\n",
        "  qns.total_quantized_steps = step_per_split\n",
        "  qns.total_time = 2 * int(step_per_split/steps_per_quarter)\n",
        "  time_signature = qns.time_signatures.add()\n",
        "  time_signature.numerator = 4\n",
        "  time_signature.denominator = 4\n",
        "  time_signature.time = 0\n",
        "  tempo = qns.tempos.add()\n",
        "  tempo.qpm = 120 # quarters per minute\n",
        "  tempo.time = 0\n",
        "  return qns\n",
        "  \n",
        "\n",
        "def split_note_sequences(note_sequence, bars_per_split=16, steps_per_quarter=4):\n",
        "  \"\"\"Splits note_sequences into new note sequence of length bars_per_split\n",
        "  \n",
        "  Given a note_sequence, we quantize it to steps_per_quarter notes in a beat. We\n",
        "  then grab 16*bar_per_split steps into separate note_sequences, assuming that \n",
        "  the bars are in 4|4 time. Setting bars_per_split to 2 will return an array of \n",
        "  note sequences that are of length 32. \n",
        "  \n",
        "  Args:\n",
        "    note_sequence:      NoteSequence, input note sequence to split\n",
        "    bars_per_split:     int, number of bars each output note sequence will have. \n",
        "    steps_per_quarter:  int, number of steps in each quarter note.\n",
        "  Return\n",
        "    np.array[NoteSequences] - output of note sequences of length \n",
        "                              bars_per_split*16\n",
        "  \"\"\"\n",
        "  quantized_seq = mm.quantize_note_sequence(note_sequence, steps_per_quarter)\n",
        "  # Copy QNS into it's own note_sequences, and clear out its notes. We \n",
        "  # are going to iterate over the notes until we hit split boundaries. \n",
        "  # Once we hit a boundary, we split any ongoing notes in two, one for\n",
        "  # the new QNS we have been collecting and one for the new one we will \n",
        "  # make. Once the bar is completed, we will createa PianorollSequence\n",
        "  # from it, and start on the next bar. \n",
        "  step_per_split = int(steps_per_quarter*4) * bars_per_split\n",
        "  split_steps = range(0, quantized_seq.total_quantized_steps, step_per_split)\n",
        "  current_split = 0\n",
        "  steps_per_quarter = quantized_seq.quantization_info.steps_per_quarter \n",
        "  qns = new_note_sequence(steps_per_quarter, step_per_split)\n",
        "  pianoroll_seqs = []\n",
        "  extended_notes = new_note_sequence(steps_per_quarter, step_per_split)\n",
        "  for note in quantized_seq.notes:\n",
        "    if note.quantized_start_step >= split_steps[current_split] + step_per_split: \n",
        "      # End of split sections. Close up QNS and set up next split.\n",
        "      pianoroll_seqs.append(copy.deepcopy(qns))\n",
        "      qns = extended_notes\n",
        "      extended_notes = new_note_sequence(steps_per_quarter, step_per_split)\n",
        "      current_split += 1\n",
        "\n",
        "    if note.quantized_end_step >= split_steps[current_split] + step_per_split:\n",
        "      # Note extends past this split. Split the note into two notes. Add note\n",
        "      # from note.start_step to end of current split. Save note from start of \n",
        "      # next split to note.end_step\n",
        "      first_half = qns.notes.add()\n",
        "      first_half.pitch = note.pitch\n",
        "      first_half.quantized_start_step = note.quantized_start_step - split_steps[current_split] \n",
        "      first_half.quantized_end_step = step_per_split\n",
        "\n",
        "      second_half = extended_notes.notes.add()\n",
        "      second_half.pitch = note.pitch\n",
        "      second_half.quantized_start_step = 0\n",
        "      second_half.quantized_end_step = note.quantized_end_step - split_steps[current_split] \n",
        "    else:\n",
        "      # This is the normal route, where we just copy the note from the\n",
        "      # sequence into the new one.\n",
        "      new_note = qns.notes.add()\n",
        "      new_note.pitch = note.pitch\n",
        "      new_note.quantized_start_step = note.quantized_start_step - split_steps[current_split] \n",
        "      new_note.quantized_end_step = note.quantized_end_step - split_steps[current_split] \n",
        "  pianoroll_seqs.append(copy.deepcopy(qns))\n",
        "\n",
        "  return pianoroll_seqs\n",
        "\n",
        "def midi_to_pianoroll(midi_filename):\n",
        "  \"\"\"Converts Midi Files into np.arrays and concats them.\n",
        "  \n",
        "  The arrays have the following types:\n",
        "  active, weights, onsets, onset_velocities, active_velocities, offsets, \n",
        "  control_changes\n",
        "  \n",
        "  Args:\n",
        "    midi_filename: string, filename\n",
        "  Returns: \n",
        "    np.array - train tensor (33, 128, 7)\n",
        "  \"\"\"\n",
        "  note_seq = mm.midi_file_to_note_sequence(midi_filename)\n",
        "  split_note_seqs = split_note_sequences(note_seq)\n",
        "  final_array = []\n",
        "  for note_seq in split_note_seqs:\n",
        "    # This outputs (33, 128, 7) It's 33 instead of 32 because it adds an end \n",
        "    # token at the end of every sequence.\n",
        "    pnt = seq_lib.sequence_to_pianoroll(\n",
        "        note_seq, 2, data.MIN_MIDI_PITCH, data.MAX_MIDI_PITCH)\n",
        "    # t_list = [pnt.active, pnt.weights, pnt.onsets, \n",
        "    #                               pnt.onset_velocities, pnt.active_velocities, \n",
        "    #                               pnt.offsets, pnt.control_changes]\n",
        "    t_list = [pnt.onsets]                                  \n",
        "    final_array.append(np.concatenate(t_list, axis=-1))\n",
        "  return np.array(final_array)\n",
        "\n",
        "def pianoroll_to_notes(pianoroll, opt_midi_file_name=\"\"):\n",
        "  \"\"\"Helper to obtain note_seq.\"\"\"\n",
        "  note_seq = seq_lib.pianoroll_to_note_sequence(pianoroll, 2, 0)\n",
        "  if opt_midi_file_name != \"\":\n",
        "    download(note_seq, opt_midi_file_name)\n",
        "  return note_seq\n",
        "\n",
        "def load_midi_data_from_midi_files(filenames):\n",
        "  \"\"\"Loads files.\n",
        "  Args:\n",
        "    filenames: list of strings, midi filenames\n",
        "  Returns: \n",
        "    pandas dataframe.\"\"\"\n",
        "  tensor_dict = collections.OrderedDict()\n",
        "  index_array = []\n",
        "  i = 0\n",
        "  for mn in filenames:\n",
        "    tensor_dict[mn]=midi_to_pianoroll(PATH_PREFIX+mn)\n",
        "    index_array.append(i)\n",
        "    i=i+1\n",
        "  return tensor_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiAVLOOcYLST",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def preprocess_music_for_vgg(music, load_dims=False):\n",
        "    global input_width, input_height, input_WIDTH, input_HEIGHT, aspect_ratio\n",
        "\n",
        "    # Expand the temporal music into a 3 channled tensor to complement VGG input\n",
        "    temp = np.zeros(music.shape + (3,), dtype=np.uint8)\n",
        "    temp[:, :, 0] = music\n",
        "    temp[:, :, 1] = music.copy()\n",
        "    temp[:, :, 2] = music.copy()\n",
        "\n",
        "    music = temp\n",
        "\n",
        "    if load_dims:\n",
        "        input_WIDTH = music.shape[0]\n",
        "        input_HEIGHT = music.shape[1]\n",
        "        aspect_ratio = float(input_HEIGHT) / input_WIDTH\n",
        "\n",
        "        input_width = music_size\n",
        "        input_height = int(input_width * aspect_ratio)\n",
        "\n",
        "    music = imresize(music, (input_width, input_height)).astype('float32')\n",
        "\n",
        "\n",
        "    music = np.expand_dims(music, axis=0)\n",
        "    return music"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPKwleFJYcIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# util function to preserve image color\n",
        "def original_color_transform(content, generated, mask=None):\n",
        "    generated = fromimage(toimage(generated, mode='RGB'), mode='YCbCr')  # Convert to YCbCr color space\n",
        "\n",
        "    if mask is None:\n",
        "        generated[:, :, 1:] = content[:, :, 1:]  # Generated CbCr = Content CbCr\n",
        "    else:\n",
        "        width, height, channels = generated.shape\n",
        "\n",
        "        for i in range(width):\n",
        "            for j in range(height):\n",
        "                if mask[i, j] == 1:\n",
        "                    generated[i, j, 1:] = content[i, j, 1:]\n",
        "\n",
        "    generated = fromimage(toimage(generated, mode='YCbCr'), mode='RGB')  # Convert to RGB color space\n",
        "    return generated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYYvg5aSYli2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pooling_func(x):\n",
        "    if pooltype == 1:\n",
        "        return AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "    else:\n",
        "        return MaxPooling2D((2, 2), strides=(2, 2))(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3k6ZMIhZQWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_inputs(content_img, style_img):\n",
        "  # Content Model\n",
        "  # get tensor representations of our images\n",
        "  content_image = midi_to_pianoroll(content_img)\n",
        "  content_image = content_image[:1,:,:]\n",
        "  play(pianoroll_to_notes(content_image))\n",
        "  style_image = midi_to_pianoroll(style_img)\n",
        "  style_image = style_image[:1,:,:]\n",
        "  play(pianoroll_to_notes(style_image))\n",
        "\n",
        "  combination_image = K.placeholder((1, 257, 128))\n",
        "  # combine the various images into a single Keras tensor\n",
        "  input_tensor = K.concatenate([content_image,style_image,\n",
        "                               combination_image], axis=0)\n",
        "      \n",
        "  return input_tensor, (3, 257, 128), combination_image, 3   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjzqMng0YoMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(input_tensor, shape):  \n",
        "  ip = Input(tensor=input_tensor, batch_shape=shape)\n",
        "\n",
        "  # build the VGG16 network with our 3 images as input\n",
        "  x = Convolution2D(64, (3, 3), activation='relu', name='conv1_1', padding='same')(ip)\n",
        "  x = Convolution2D(64, (3, 3), activation='relu', name='conv1_2', padding='same')(x)\n",
        "  x = pooling_func(x)\n",
        "\n",
        "  x = Convolution2D(128, (3, 3), activation='relu', name='conv2_1', padding='same')(x)\n",
        "  x = Convolution2D(128, (3, 3), activation='relu', name='conv2_2', padding='same')(x)\n",
        "  x = pooling_func(x)\n",
        "\n",
        "  x = Convolution2D(256, (3, 3), activation='relu', name='conv3_1', padding='same')(x)\n",
        "  x = Convolution2D(256, (3, 3), activation='relu', name='conv3_2', padding='same')(x)\n",
        "  x = Convolution2D(256, (3, 3), activation='relu', name='conv3_3', padding='same')(x)\n",
        "  if model == \"vgg19\":\n",
        "      x = Convolution2D(256, (3, 3), activation='relu', name='conv3_4', padding='same')(x)\n",
        "  x = pooling_func(x)\n",
        "\n",
        "  x = Convolution2D(512, (3, 3), activation='relu', name='conv4_1', padding='same')(x)\n",
        "  x = Convolution2D(512, (3, 3), activation='relu', name='conv4_2', padding='same')(x)\n",
        "  x = Convolution2D(512, (3, 3), activation='relu', name='conv4_3', padding='same')(x)\n",
        "  if model == \"vgg19\":\n",
        "      x = Convolution2D(512, (3, 3), activation='relu', name='conv4_4', padding='same')(x)\n",
        "  x = pooling_func(x)\n",
        "\n",
        "  x = Convolution2D(512, (3, 3), activation='relu', name='conv5_1', padding='same')(x)\n",
        "  x = Convolution2D(512, (3, 3), activation='relu', name='conv5_2', padding='same')(x)\n",
        "  x = Convolution2D(512, (3, 3), activation='relu', name='conv5_3', padding='same')(x)\n",
        "  if model == \"vgg19\":\n",
        "      x = Convolution2D(512, (3, 3), activation='relu', name='conv5_4', padding='same')(x)\n",
        "  x = pooling_func(x)\n",
        "\n",
        "  return Model(ip, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ1hseTF4r1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_custom_model(input_tensor, shape, use_batchnorm=False, use_dropout=False):\n",
        "  ip = Input(tensor=input_tensor, batch_shape=shape)\n",
        "  layer_2 = ip\n",
        "  if use_batchnorm:\n",
        "    layer_2 = BatchNormalization(axis=-1)(layer_2)\n",
        "  layer_2 = Convolution1D(filters=64, kernel_size=32, name=\"conv1\", activation='elu', padding='same')(layer_2)\n",
        "  if use_dropout:\n",
        "    layer_2 = Dropout(rate=0.1)(layer_2)\n",
        "  layer_2 = Convolution1D(filters=16, kernel_size=8, name=\"conv2\", activation='elu', padding='same')(layer_2)\n",
        "  layer_2 = Convolution1D(filters=1, kernel_size=4, name=\"conv3\", activation='elu', padding='same')(layer_2)\n",
        "  layer_2 = Lambda(lambda x: backend.squeeze(x, axis=-1))(layer_2)\n",
        "  dense_1 = Dense(units=256, name=\"dense1\", activation='elu')(layer_2)\n",
        "  if use_dropout:\n",
        "    layer_2 = Dropout(rate=0.1)(layer_2)\n",
        "  dense_1 = Dense(units=32, name=\"dense2\", activation='elu')(dense_1)\n",
        "  dense_1 = Dense(units=NUM_UNIQUE_COMPOSERS, name=\"dense3\", activation='elu')(dense_1)\n",
        "  dense_1 = Dense(units=NUM_UNIQUE_COMPOSERS, name=\"output\", activation='softmax')(dense_1)\n",
        "  return Model(ip, dense_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAER915hZ_XC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(vgg_model):\n",
        "  if model == \"vgg19\":\n",
        "      weights = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', TF_19_WEIGHTS_PATH_NO_TOP, cache_subdir='models')\n",
        "  else:\n",
        "      weights = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', TF_WEIGHTS_PATH_NO_TOP, cache_subdir='models')\n",
        "\n",
        "  vgg_model.load_weights(weights)\n",
        "\n",
        "  if K.backend() == 'tensorflow' and K.image_dim_ordering() == \"th\":\n",
        "      warnings.warn('You are using the TensorFlow backend, yet you '\n",
        "                    'are using the Theano '\n",
        "                    'image dimension ordering convention '\n",
        "                    '(`image_dim_ordering=\"th\"`). '\n",
        "                    'For best performance, set '\n",
        "                    '`image_dim_ordering=\"tf\"` in '\n",
        "                    'your Keras config '\n",
        "                    'at ~/.keras/keras.json.')\n",
        "      convert_all_kernels_in_model(vgg_model)\n",
        "  print('Model loaded.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbT4XCzjaYw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_output_dict_and_shape(vgg_model):\n",
        "  # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "  outputs_dict = dict([(layer.name, layer.output) for layer in vgg_model.layers])\n",
        "  shape_dict = dict([(layer.name, layer.output_shape) for layer in vgg_model.layers])\n",
        "  return outputs_dict, shape_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcveYQXBaoxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the gram matrix of an image tensor (feature-wise outer product)\n",
        "def gram_matrix(x):\n",
        "#     assert K.ndim(x) == 3\n",
        "    features = x\n",
        "    if K.ndim(x) == 3:\n",
        "      if K.image_dim_ordering() == \"th\":\n",
        "          features = K.batch_flatten(x)\n",
        "      else:\n",
        "          features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "    if K.ndim(x) == 2:\n",
        "      return K.dot(features, K.transpose(features))\n",
        "    return features * features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0Pe48F9ayTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the \"style loss\" is designed to maintain\n",
        "# the style of the reference image in the generated image.\n",
        "# It is based on the gram matrices (which capture style) of\n",
        "# feature maps from the style reference image\n",
        "# and from the generated image\n",
        "def style_loss(style, combination, mask_path=None, nb_channels=None):\n",
        "#     assert K.ndim(style) == 3\n",
        "#     assert K.ndim(combination) == 3\n",
        "\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    channels = 3\n",
        "    size = input_width * input_height\n",
        "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG40aZVab0aQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# an auxiliary loss function\n",
        "# designed to maintain the \"content\" of the\n",
        "# base image in the generated image\n",
        "def content_loss(base, combination):\n",
        "    channel_dim = 0 if K.image_dim_ordering() == \"th\" else -1\n",
        "\n",
        "    try:\n",
        "        channels = K.int_shape(base)[channel_dim]\n",
        "    except TypeError:\n",
        "        channels = K.shape(base)[channel_dim]\n",
        "    size = input_width * input_height\n",
        "\n",
        "    if content_loss_type == 1:\n",
        "        multiplier = 1. / (2. * (channels ** 0.5) * (size ** 0.5))\n",
        "    elif content_loss_type == 2:\n",
        "        multiplier = 1. / (channels * size)\n",
        "    else:\n",
        "        multiplier = 1.\n",
        "\n",
        "    return multiplier * K.sum(K.square(combination - base))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar7ZQltIb0u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the 3rd loss function, total variation loss,\n",
        "# designed to keep the generated image locally coherent\n",
        "def total_variation_loss(x):\n",
        "    a = K.square(x[:, :input_width - 1, :input_height - 1] - x[:, 1:, :input_height - 1])\n",
        "    b = K.square(x[:, :input_width - 1, :input_height - 1] - x[:, :input_width - 1, 1:])\n",
        "    return K.sum(K.pow(a + b, 1.25))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke5TMvafcPXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss(outputs_dict, shape_dict, combination_image, nb_tensors):  \n",
        "  # combine these loss functions into a single scalar\n",
        "  loss = K.variable(0.)\n",
        "  layer_features = outputs_dict[content_layer]  # 'conv5_2' or 'conv4_2'\n",
        "  base_image_features = layer_features[0, :]\n",
        "  combination_features = layer_features[nb_tensors - 1, :]\n",
        "  loss = loss + content_weight * content_loss(base_image_features,\n",
        "                                        combination_features)\n",
        "  style_masks = [None] # If masks not present, pass None to the style loss\n",
        "\n",
        "  channel_index = 1 if K.image_dim_ordering() == \"th\" else -1\n",
        "  feature_layers = ['conv1', 'conv2', 'conv3', 'dense1', 'dense2', 'dense3']\n",
        "  for layer_name in feature_layers:\n",
        "      layer_features = outputs_dict[layer_name]\n",
        "      shape = shape_dict[layer_name]\n",
        "      combination_features = []\n",
        "      style_reference_features = []\n",
        "      if 'conv' in layer_name:\n",
        "        combination_features = layer_features[nb_tensors - 1, :, :]\n",
        "        style_reference_features = layer_features[1:nb_tensors - 1, :, :]\n",
        "      else:\n",
        "        combination_features = layer_features[nb_tensors - 1, :]\n",
        "        style_reference_features = layer_features[1:nb_tensors - 1, :]\n",
        "      sl = []\n",
        "      sl.append(style_loss(style_reference_features[0], combination_features, style_masks[0], shape))\n",
        "      loss = loss + (1 / len(feature_layers)) * sl[0]\n",
        "\n",
        "  loss = loss + total_variation_weight * total_variation_loss(combination_image)\n",
        "  return loss\n",
        "  \n",
        "def get_grad(loss, combination_image):\n",
        "  # get the gradients of the generated image wrt the loss\n",
        "  grads = K.gradients(loss, combination_image)\n",
        "  outputs = [loss]\n",
        "  if type(grads) in {list, tuple}:\n",
        "      outputs += grads\n",
        "  else:\n",
        "      outputs.append(grads)\n",
        "  return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtAtBwIwgGTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_loss_and_grads(x, f_outputs):\n",
        "    x = x.reshape((1, 257, 128))\n",
        "    outs = f_outputs([x])\n",
        "    loss_value = outs[0]\n",
        "    if len(outs[1:]) == 1:\n",
        "        grad_values = outs[1].flatten().astype('float64')\n",
        "    else:\n",
        "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
        "    return loss_value, grad_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0hmGQVPgG6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this Evaluator class makes it possible\n",
        "# to compute loss and gradients in one pass\n",
        "# while retrieving them via two separate functions,\n",
        "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
        "# requires separate functions for loss and gradients,\n",
        "# but computing them separately would be inefficient.\n",
        "class Evaluator(object):\n",
        "    def __init__(self, f_outputs):\n",
        "        self.loss_value = None\n",
        "        self.grads_values = None\n",
        "        self.f_outputs = f_outputs\n",
        "\n",
        "    def loss(self, x):\n",
        "        assert self.loss_value is None\n",
        "        loss_value, grad_values = eval_loss_and_grads(x, self.f_outputs)\n",
        "        self.loss_value = loss_value\n",
        "        self.grad_values = grad_values\n",
        "        return self.loss_value\n",
        "\n",
        "    def grads(self, x):\n",
        "        assert self.loss_value is not None\n",
        "        grad_values = np.copy(self.grad_values)\n",
        "        self.loss_value = None\n",
        "        self.grad_values = None\n",
        "        return grad_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HcB2v9d5weX",
        "colab_type": "text"
      },
      "source": [
        "# MIDI time\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmLdhGyf5_so",
        "colab_type": "code",
        "outputId": "7e167e85-e4df-4980-ee3c-8be8244926cd",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "style_music = \"MIDI-Unprocessed_SMF_02_R1_2004_01-05_ORIG_MID--AUDIO_02_R1_2004_05_Track05_wav.midi\"\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7abc66b6-e783-4a67-9db2-23ebae432611\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7abc66b6-e783-4a67-9db2-23ebae432611\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUhd6oglf7Vg",
        "colab_type": "code",
        "outputId": "3bc2cd65-22f8-4458-cae9-6f3311f256a1",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "content_music = \"MIDI-Unprocessed_083_PIANO083_MID--AUDIO-split_07-09-17_Piano-e_2_-06_wav--5.midi\"\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e7ae1570-b66e-4305-812a-956ddb1f69bd\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e7ae1570-b66e-4305-812a-956ddb1f69bd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgoGGxAbfaLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmCAZbSqB9In",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_music(path):\n",
        "  midi_pretty_format = pretty_midi.PrettyMIDI(path)\n",
        "  piano_midi = midi_pretty_format.instruments[0] # Get the piano channels\n",
        "  piano_roll = piano_midi.get_piano_roll(fs=5)\n",
        "  piano_roll=piano_roll[:,:256]\n",
        "  return piano_roll"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69Zsvlawq_20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def piano_roll_to_pretty_midi(piano_roll, fs=5, program=0):\n",
        "    '''Convert a Piano Roll array into a PrettyMidi object\n",
        "     with a single instrument.\n",
        "    Parameters\n",
        "    ----------\n",
        "    piano_roll : np.ndarray, shape=(128,frames), dtype=int\n",
        "        Piano roll of one instrument\n",
        "    fs : int\n",
        "        Sampling frequency of the columns, i.e. each column is spaced apart\n",
        "        by ``1./fs`` seconds.\n",
        "    program : int\n",
        "        The program number of the instrument.\n",
        "    Returns\n",
        "    -------\n",
        "    midi_object : pretty_midi.PrettyMIDI\n",
        "        A pretty_midi.PrettyMIDI class instance describing\n",
        "        the piano roll.\n",
        "    '''\n",
        "    notes, frames = piano_roll.shape\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    instrument = pretty_midi.Instrument(program=program)\n",
        "\n",
        "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
        "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
        "\n",
        "    # use changes in velocities to find note on / note off events\n",
        "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
        "\n",
        "    # keep track on velocities and note on times\n",
        "    prev_velocities = np.zeros(notes, dtype=int)\n",
        "    note_on_time = np.zeros(notes)\n",
        "\n",
        "    for time, note in zip(*velocity_changes):\n",
        "        # use time + 1 because of padding above\n",
        "        velocity = piano_roll[note, time + 1]\n",
        "        time = time / fs\n",
        "        if velocity > 0:\n",
        "            if prev_velocities[note] == 0:\n",
        "                note_on_time[note] = time\n",
        "                prev_velocities[note] = velocity\n",
        "        else:\n",
        "            pm_note = pretty_midi.Note(\n",
        "                velocity=prev_velocities[note],\n",
        "                pitch=note,\n",
        "                start=note_on_time[note],\n",
        "                end=time)\n",
        "            instrument.notes.append(pm_note)\n",
        "            prev_velocities[note] = 0\n",
        "    pm.instruments.append(instrument)\n",
        "    return pm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuIYxIpXtpkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deprocess_music(x):\n",
        "    x = x.reshape((257, 128))\n",
        "    notes = pianoroll_to_notes(x)\n",
        "    return notes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpNsXIeUFeTy",
        "colab_type": "code",
        "outputId": "aa9adabd-f5b2-4a43-bfdb-dd38d676a187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Create embedding layer to squeeze to dimensions to 16\n",
        "c_music = get_music(content_music)\n",
        "s_music = get_music(style_music)\n",
        "print(c_music.shape)\n",
        "print(s_music.shape)\n",
        "print(c_music[:, :30].max(axis=0))\n",
        "print(c_music[:, 5])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 256)\n",
            "(128, 256)\n",
            "[ 0.  0.  0.  0.  0. 37.  0.  0.  0. 40. 40. 40. 40. 40.  0. 43. 43. 43.\n",
            " 43. 38.  0.  0.  0.  0.  0.  0. 33.  0.  0.  0.]\n",
            "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0. 37.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sby39810JOSt",
        "colab_type": "text"
      },
      "source": [
        "#Run Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXDX_sKmW13N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "outputId": "df666ce6-6e81-4c80-8032-e6b0d0d67506"
      },
      "source": [
        "input_tensor, shape, combination_image, nb_tensors = get_inputs(content_music, style_music)\n",
        "vgg_model = get_custom_model(input_tensor, shape)\n",
        "vgg_model.load_weights(\"model_best_1.h5\")\n",
        "vgg_model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-a0b14056bad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombination_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_music\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_music\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvgg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_custom_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvgg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_best_1.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvgg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-4966c6f49e23>\u001b[0m in \u001b[0;36mget_inputs\u001b[0;34m(content_img, style_img)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mcontent_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmidi_to_pianoroll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mcontent_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpianoroll_to_notes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mstyle_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmidi_to_pianoroll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mstyle_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstyle_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-9a76ffa39cf1>\u001b[0m in \u001b[0;36mpianoroll_to_notes\u001b[0;34m(pianoroll, opt_midi_file_name)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpianoroll_to_notes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpianoroll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_midi_file_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;34m\"\"\"Helper to obtain note_seq.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m   \u001b[0mnote_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpianoroll_to_note_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpianoroll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mopt_midi_file_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_midi_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/magenta/music/sequences_lib.pyc\u001b[0m in \u001b[0;36mpianoroll_to_note_sequence\u001b[0;34m(frames, frames_per_second, min_duration_ms, velocity, instrument, program, qpm, min_midi_pitch, onset_predictions, offset_predictions, velocity_values)\u001b[0m\n\u001b[1;32m   1905\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1907\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mactive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1908\u001b[0m         \u001b[0mprocess_active_pitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mpitch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpitch_start_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpZXqsB5JHFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs_dict, shape_dict = get_output_dict_and_shape(vgg_model)\n",
        "loss = get_loss(outputs_dict, shape_dict, combination_image, nb_tensors)\n",
        "outputs = get_grad(loss, combination_image)\n",
        "f_outputs = K.function([combination_image], outputs)\n",
        "evaluator = Evaluator(f_outputs)\n",
        "\n",
        "prev_min_val = -1\n",
        "\n",
        "# x = preprocess_music_for_vgg(get_music(content_music))\n",
        "x = midi_to_pianoroll(content_music)\n",
        "x = x[:1,:,:]\n",
        "improvement_threshold = float(min_improvement)\n",
        "\n",
        "result_prefix = \"midi_straight\"\n",
        "\n",
        "loss_aray1= []\n",
        "\n",
        "for i in range(num_iter):\n",
        "    print(\"Starting iteration %d of %d\" % ((i + 1), num_iter))\n",
        "    start_time = time.time()\n",
        "\n",
        "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20)\n",
        "    \n",
        "    loss_aray1.append(min_val)\n",
        "    if prev_min_val == -1:\n",
        "        prev_min_val = min_val\n",
        "\n",
        "    improvement = (prev_min_val - min_val) / prev_min_val * 100\n",
        "    \n",
        "\n",
        "    print('Current loss value:', min_val, \" Improvement : %0.3f\" % improvement, \"%\")\n",
        "    prev_min_val = min_val\n",
        "    # save current generated image\n",
        "    img = deprocess_music(x.copy())\n",
        "\n",
        "#     if not rescale_image:\n",
        "#         img_ht = int(input_width * aspect_ratio)\n",
        "#         print(\"Rescaling Image to (%d, %d)\" % (input_width, img_ht))\n",
        "#         img = imresize(img, (input_width, img_ht), interp=rescale_method)\n",
        "\n",
        "#     if rescale_image:\n",
        "#         print(\"Rescaling Image to (%d, %d)\" % (input_WIDTH, input_HEIGHT))\n",
        "#         img = imresize(img, (input_WIDTH, input_HEIGHT), interp=rescale_method)\n",
        "\n",
        "    fname = result_prefix + '_at_iteration_%d.midi' % (i + 1)\n",
        "    img = note_sequence_to_pretty_midi(img)\n",
        "    img.write(fname)\n",
        "    end_time = time.time()\n",
        "    print('Image saved as', fname)\n",
        "    print('Iteration %d completed in %ds' % (i + 1, end_time - start_time))\n",
        "\n",
        "    if improvement_threshold is not 0.0:\n",
        "        if improvement < improvement_threshold and improvement is not 0.0:\n",
        "            print(\"Improvement (%f) is less than improvement threshold (%f). Early stopping script.\" % (\n",
        "                improvement, improvement_threshold))\n",
        "            exit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhq1rhmxMJep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH_XrtjRjsXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWu-OK5bjpaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(\"midi_straight_at_iteration_1.midi\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK35GovB0iE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss_aray1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6PNmEZEBZSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}